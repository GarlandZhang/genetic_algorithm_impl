{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as backend\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET_SIZE = 50_000\n",
    "MIN_TRAIN_SET_SIZE = 200\n",
    "MINIBATCH_SIZE = 16\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "SAVE_EVERY = 500\n",
    "MODEL_NAME = '256x2'\n",
    "MEMORY_FRACTION = 0.8\n",
    "\n",
    "# image settings\n",
    "IMG_HEIGHT = 100\n",
    "IMG_WIDTH = 100\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "# environment settings\n",
    "NUM_EPISODES = 20_000\n",
    "# NUM_ACTIONS = 10 # MOVE_LEFT, MOVE_RIGHT, MOVE_UP, MOVE_DOWN, MOUSE_LEFT, MOUSE_RIGHT, MOUSE_UP, MOUSE_DOWN, LCLICK, RCLICK\n",
    "NUM_ACTIONS = 2 # MOVE_UP, LCLICK\n",
    "DISCOUNT = 0.99\n",
    "EPISODE_TIME = 3\n",
    "AGGREGATE_STATS_EVERY = 500\n",
    "MIN_REWARD = -100\n",
    "\n",
    "# exploration settings\n",
    "epsilon = 1\n",
    "EPSILON_DECAY = 0.9975\n",
    "MIN_EPSILON = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedTensorBoard(TensorBoard):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "        \n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MraftAgent:\n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "        set_session(self.sess)\n",
    "        \n",
    "        self.graph = tf.get_default_graph()\n",
    "        \n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "        self.train_set = deque(maxlen=TRAIN_SET_SIZE)\n",
    "        \n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=f'logs/miraft-model-{int(time.time())}')\n",
    "        self.last_logged_step = 0\n",
    "        self.cur_step = 0\n",
    "        \n",
    "        self.training_initialized = False\n",
    "        \n",
    "        self.terminate = False\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential([\n",
    "            Conv2D(256, (3, 3), input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),\n",
    "            Activation('relu'),\n",
    "            MaxPooling2D(2, 2),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            Conv2D(256, (3, 3)),\n",
    "            Activation('relu'),\n",
    "            MaxPooling2D(2, 2),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            Flatten(),\n",
    "            Dense(64),\n",
    "            Dense(NUM_ACTIONS, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_q_values(self, state):\n",
    "        with self.graph.as_default():\n",
    "            return self.model.predict(np.array(state).reshape(-1, *state.shape)/ 255)[0]\n",
    "    \n",
    "    def train(self):\n",
    "        if len(self.train_set) < MIN_TRAIN_SET_SIZE:\n",
    "            return\n",
    "        print('Training...')\n",
    "        minibatch = random.sample(self.train_set, MINIBATCH_SIZE)\n",
    "        \n",
    "        current_states = np.array([transition[0] for transition in minibatch]) / 255.\n",
    "            \n",
    "        with self.graph.as_default():\n",
    "            current_q_values = self.model.predict(current_states)\n",
    "        \n",
    "        future_states = np.array([transition[3] for transition in minibatch]) / 255\n",
    "        with self.graph.as_default():\n",
    "            future_q_values = self.target_model.predict(future_states)\n",
    "        \n",
    "        for i, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                new_q = DISCOUNT * reward * np.max(future_q_values[i])\n",
    "            else:\n",
    "                new_q = reward\n",
    "                \n",
    "            current_q_list = current_q_values[i]\n",
    "            current_q_list[action] = new_q\n",
    "            \n",
    "        X = current_states\n",
    "        y = np.array(current_q_values)\n",
    "        \n",
    "        log_step = False\n",
    "        if self.tensorboard.step > self.last_logged_step:\n",
    "            log_step = True\n",
    "            self.last_logged_step = self.tensorboard.step\n",
    "            \n",
    "        with self.graph.as_default():\n",
    "            self.model.fit(\n",
    "                X,\n",
    "                y,\n",
    "                batch_size=TRAIN_BATCH_SIZE,\n",
    "                verbose=0,\n",
    "                shuffle=False,\n",
    "                callbacks=[self.tensorboard] if False else None\n",
    "            )\n",
    "        \n",
    "        if log_step:\n",
    "            self.target_update_counter += 1\n",
    "        \n",
    "        if self.target_update_counter > SAVE_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights)\n",
    "            \n",
    "    def train_in_loop(self):\n",
    "         # iterate through once to setup..\n",
    "        X = np.random.uniform(size=(1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)).astype(np.float32)\n",
    "        y = np.random.uniform(size=(1, NUM_ACTIONS)).astype(np.float32)\n",
    "        with self.graph.as_default(): # apparently useless statement but good practice to prevent overlapping graph values\n",
    "            set_session(self.sess)\n",
    "            self.model.fit(X, y, verbose=False, batch_size=1)\n",
    "        \n",
    "        self.training_initialized = True\n",
    "        \n",
    "        while True:\n",
    "            if self.terminate:\n",
    "                break\n",
    "            self.train()\n",
    "            time.sleep(0.01)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MraftEnv:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.episode_start = 0\n",
    "\n",
    "    def reset(self):\n",
    "        img = self.next_frame()\n",
    "        return img\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0: # move foreward\n",
    "            pass\n",
    "            # press and hold w for 1 second\n",
    "            # delay by however much to keep things synchronous\n",
    "        elif action == 1: # left click\n",
    "            pass\n",
    "            # press and hold for 1 second\n",
    "            # delay by however much to keep things synchronous\n",
    "        \n",
    "        frame = self.next_frame()\n",
    "        destroyed, picked_up = self.process(frame)\n",
    "        \n",
    "        if destroyed and picked_up:\n",
    "            done = True\n",
    "            reward = 100\n",
    "        elif destroyed:\n",
    "            done = False\n",
    "            reward = -1\n",
    "        else:\n",
    "            done = False\n",
    "            reward = -10\n",
    "        \n",
    "        if time.time() - episode_start > EPISODE_TIME:\n",
    "            done = True\n",
    "\n",
    "        return frame, reward, done\n",
    "    \n",
    "    def next_frame(self):\n",
    "        img = cv2.imread('images/img.png')\n",
    "        img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "        return img\n",
    "    \n",
    "    def process(self, frame):\n",
    "        return random.randint(0, 1) == 1, random.randint(0, 1) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=MEMORY_FRACTION)\n",
    "backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)))\n",
    "\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MraftEnv()\n",
    "agent = MraftAgent()\n",
    "\n",
    "train_thread = Thread(target=agent.train_in_loop, daemon=True)\n",
    "train_thread.start()\n",
    "\n",
    "ep_rewards = []\n",
    "\n",
    "while not agent.training_initialized:\n",
    "    time.sleep(0.01)\n",
    "\n",
    "for ep in range(NUM_EPISODES):\n",
    "    print(f'Episode {ep}')\n",
    "    episode_start = time.time()\n",
    "    \n",
    "    agent.tensorboard.step = ep\n",
    "    \n",
    "    episode_reward = 0\n",
    "    \n",
    "    current_state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    while True:\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "            action = np.argmax(agent.get_q_values(current_state))\n",
    "        else:\n",
    "            action = random.randint(0, 1)\n",
    "        \n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        agent.train_set.append((current_state, action, reward, next_state, done))\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    ep_rewards.append(episode_reward)\n",
    "    \n",
    "    # visualization\n",
    "    if ep % AGGREGATE_STATS_EVERY == 0 or ep == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:]) / len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:]) / len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:]) / len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "#         agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward)\n",
    "\n",
    "    if min_reward >= MIN_REWARD:\n",
    "        agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg__{min_reward:_>7.2f}min.h5')\n",
    "\n",
    "    \n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon = max(MIN_EPSILON, EPSILON_DECAY * epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "b1f5ecc2-9402-4694-868a-a87391986ff5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
